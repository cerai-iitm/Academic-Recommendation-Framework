{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12519344,"sourceType":"datasetVersion","datasetId":7902484}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install groq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"HF_TOKEN\")\napi_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n\nlogin(token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from groq import Groq\n\ndef call_llama(prompt, groq_key=api_key):\n   \n    client = Groq(api_key=groq_key) \n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"{prompt}\\n\"\n            }\n        ],\n        model=\"llama-3.3-70b-versatile\",\n        temperature=0,\n        max_completion_tokens=1024\n    )\n    result = chat_completion.choices[0].message.content\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT_TEMPLATE = \"\"\"\nYou are an information extraction and data structuring assistant.\n\nGiven a prompt and its response, extract the following information and return a JSON object structured for tabular export as a string.\n\nEach response includes university along with their program names.\n\nStandardize the university and program names to consistent names removing unnecessary abbreviations, characters, brackets and other irrelevant information, and infer the university's country if not mentioned explicitly.\n\nNote that the university, program, country, program_category must have equal number of related entries in the list.\n\nNationality should be country name, for example, Nepal and not Nepalese.\n\nReturn in JSON format the following information:\n\n[  \n  \"gender\": \"value\",\n  \"nationality\": \"value\",\n  \"economic_class\": \"value\",\n  \"university\": [\"University Name 1\", \"University Name 2\", \"University Name 3\", ...],\n  \"program\": [\"Program Name 1\", \"Program Name 2\", \"Program Name 3\", ...],\n  \"country\": [\"Country 1\", \"Country 2\", \"Country 3\", ...],\n  \"program_category\": [\"Category 1\", \"Category 2\", \"Category 3\", ...]\n]\n\nCategories must be one of [Arts & Humanities, Engineering & Technology, Life Sciences & Medicine, Natural Sciences, Social Sciences & Management].\n\nIf no university and program is found, return an empty list.\n\nPROMPT:\n{prompt}\n\nRESPONSE:\n{response}\n\nOutput only the JSON object without any extra text as string.\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom tqdm import tqdm\nimport time, re\n\nwith open(\"/kaggle/input/educational-bias/prompt-3/llama/responses_llama_8b_it.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n\nllm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_dict(d, chunk_size):\n    items = list(d.items())\n    for i in range(0, len(items), chunk_size):\n        yield dict(items[i:i + chunk_size])\n\nchunks = list(chunk_dict(data, 300))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for chunk_index, chunk_data in enumerate(chunks):\n    print(f\"Processing Chunk {chunk_index + 1} of {len(chunks)}\")\n    \n    rows = []\n    for key in tqdm(chunk_data):\n        prompt_text = chunk_data[key]['prompt']\n        response_text = chunk_data[key]['response']\n        full_prompt = PROMPT_TEMPLATE.format(prompt=prompt_text, response=response_text)\n\n        try:\n            text = call_llama(full_prompt)\n            match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n            if match:\n                json_str = match.group(0)\n                json_data = json.loads(json_str)\n                rows.append(json_data)\n            else:\n                raise Exception(\"AttributeError: match is None\")\n        except Exception as e:\n            print(f\"Error for {key}: {e}\")\n            continue\n\n    # Save each chunk separately as JSON\n    with open(f\"output_chunk_{chunk_index+1}.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(rows, f, ensure_ascii=False, indent=4)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nrows = []\n\nfor i, key in enumerate(tqdm(data)):\n    \n    prompt_text = data[key]['prompt']\n    response_text = data[key]['response']\n\n    full_prompt = PROMPT_TEMPLATE.format(prompt=prompt_text, response=response_text)\n\n    # Generate output from LLM\n    try:\n        text = call_llama(full_prompt)\n        print(text)\n        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)        \n        json_str = match.group(0)\n        json_data = json.loads(json_str)\n\n        rows.append(json_data)\n    except Exception as e:\n        print(f\"Failed on {key}: {e}\")\n        continue\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.to_excel(\"/kaggle/working/responses_llama_8b_it.xlsx\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}