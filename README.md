# Academic-Recommendation-Framework

This repository provides a complete pipeline to **evaluate fairness and bias** in university recommendations generated by Large Language Models (LLMs). It includes synthetic user profiles, prompting strategies, and a custom evaluation framework to compute Demographic and Geographic Representation Scores.

The pipeline is based on our paper titled **"Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations"**. Preprint available [here](https://arxiv.org/abs/2509.04498)

## Background

LLMs like LLaMA, Gemma, and Mistral are increasingly used in advising students for higher education. However, their recommendations often reflect systemic global and societal biases. This project provides a reproducible evaluation framework that breaks down and quantifies such bias through a structured lens.

Our paper _"Where Should I Study? Biased LMs Decide!"_ proposes novel metrics - **Demographic Representation Score (DRS)** and **Geographic Representation Score (GRS)** to evaluate whether recommendations are aligned with user needs and globally diverse.

##  Repository Structure

```text
.
â”œâ”€â”€ Geobias_calc.ipynb         # Main notebook for computing DRS & GRS metrics
â”œâ”€â”€ query_llms.ipynb           # Prompts synthetic profiles to selected LLMs and collects results
â”œâ”€â”€ parse_model_outputs.ipynb  # Cleans and structures model outputs into CSV format
â”œâ”€â”€ requirements.txt           # Python package requirements
â””â”€â”€ README.md
```

##  Setup Instructions

Follow these steps to set up your environment and run the bias evaluation pipeline.

### Clone the repository

```bash
git clone https://github.com/yourusername/Academic-Recommendation-Framework.git
cd Academic-Recommendation-Framework
```

### Install dependencies
Make sure you have Python 3.10 or above. Then install all necessary packages:

```bash
pip install -r requirements.txt
```

### Run the Pipeline
You should execute the notebooks in the following order for a complete end-to-end run:

1. Generate Model Recommendations: query_llms.ipynb
Loads or constructs 360 synthetic profiles.
Sends structured queries to your selected LLMs (e.g., LLaMA-3.1, Gemma-7B, Mistral-7B).
Stores their university and program recommendations.

2. Parse Model Outputs: parse_model_outputs.ipynb
Cleans and structures the raw JSON or text responses.

3. Evaluate Bias and Fairness: Geobias_calc.ipynb
This is the core analysis notebook. It:
Computes Demographic Representation Score (DRS) and its components: Accessibility, Reputation, and Academic Fit.
Computes Geographic Representation Score (GRS) using country-wise diversity and reputational coverage.
Converts them into structured CSVs suitable for metric calculation.

## Evaluation Metrics

We compute two core scores for each profile-model combination:

### Demographic Representation Score (DRS)

This metric evaluates how well the university recommendations align with a student's context and background. It consists of three components:

- **Accessibility (Acc)**: How geographically and economically accessible is the recommended institution?
- **Reputation (Rep)**: Is the university well-ranked globally?
- **Academic Fit (Acad)**: Does the university offer programs that match the student's interests?

> **Final DRS** = Weighted Mean of (Acc, Rep, Acad)

A high DRS indicates that the LLM tailored its recommendation appropriately for the user.

---

### Geographic Representation Score (GRS)

This metric evaluates the country-level diversity and quality of recommended universities. It includes:

- **Representation**: What fraction of a country's universities were recommended?
- **Reputational Coverage**: Are the most reputed universities in that country being recommended?

> **Final GRS** = Geometric Mean of (Normalized Representation, Reputational Coverage)

A high GRS suggests that the model is not only recommending globally but also choosing good institutions from each region.

---

## ðŸ“ˆ Sample Output

The `Geobias_calc.ipynb` notebook generates:

- Bias scores (DRS and GRS) disaggregated by gender, nationality, and class  
- Comparisons between models (LLaMA, Gemma, Mistral)  
- Insights into trade-offs between accessibility vs prestige

---
